{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# For course instructors\n",
    "\n",
    "Whole project is available no [github](https://github.com/maxi7524/SAD2_final_project) where is readme with instruction how install dependencies.\n",
    "\n",
    "Data which we generated and used, can be download from script (readme) or be directly accessed on [google drive](https://drive.google.com/drive/folders/1fbIJiBIGMP9HivtoPEkko7hT6t2mXH0n?fbclid=IwY2xjawPkSbtleHRuA2FlbQIxMQBzcnRjBmFwcF9pZAEwAAEeeGVO2xSm_ZYKfOcl1NswUrAK5gGbMfuvn9tYDFeomyfS9i9nIgccMeW8Ooo_aem_mPHWJ7MRVySCLerEKXC-4g).\n",
    "\n",
    "To run script you only need to run `uv` and download data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "## system\n",
    "from pathlib import Path\n",
    "import os\n",
    "## data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "## analysis\n",
    "import statsmodels.formula.api as smf\n",
    "## plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## local library\n",
    "### load data\n",
    "from sad2_final_project.analysis import loader_obsolete_data, experiment_data_loader, add_missing_metrics_from_experiment, analyze_datasets_from_index\n",
    "### general plots\n",
    "from sad2_final_project.analysis import plot_scatter, plot_scatter_subplots, plot_boxplot, plot_grouped_boxplots\n",
    "### Statistical analysis and associated plots\n",
    "from sad2_final_project.analysis import compute_spearman_table, plot_spearman_heatmap, compute_wilcoxon_table, plot_wilcoxon_heatmap\n",
    "# Path management\n",
    "## set cwdir to repository folder \n",
    "cwd=Path.cwd()\n",
    "if cwd.name == \"notebooks\":\n",
    "    os.chdir(cwd.parent)\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "## create paths\n",
    "DATA_PATH = Path('data')\n",
    "DATASET_1_PATH = DATA_PATH / Path('experiment_12_merged')\n",
    "DATASET_2_S_PATH = DATA_PATH / Path('experiment_34_synchronous_merged')\n",
    "DATASET_2_A_PATH = DATA_PATH / Path('experiment_34_asynchronous_merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading \n",
    "## Dataset 1 - experiments 1, 2\n",
    "### load with column drop\n",
    "df = experiment_data_loader(DATASET_1_PATH).drop(columns=['attractor_ratio_y'])\n",
    "### rename columns\n",
    "df = df.rename(columns={'attractor_ratio_x': 'attractor_ratio'})\n",
    "## Dataset 2 - experiments 3, 4\n",
    "dfs = experiment_data_loader(DATASET_2_S_PATH).drop(columns=['part_x', 'part_y'])\n",
    "dfa = experiment_data_loader(DATASET_2_A_PATH).drop(columns=['part_x', 'part_y'])\n",
    "\n",
    "## Add all metrics\n",
    "metrics_list=['TP', 'FP', 'FN', 'f1', 'accuracy', 'precision', 'recall', 'sensitivity', 'AHD', 'SHD', 'EHD', 'SID']\n",
    "df = add_missing_metrics_from_experiment(df, DATASET_1_PATH, metrics_list, after_column='attractor_ratio')\n",
    "dfs = add_missing_metrics_from_experiment(dfs, DATASET_2_S_PATH, metrics_list, after_column='attractor_ratio')\n",
    "dfa = add_missing_metrics_from_experiment(dfa, DATASET_2_A_PATH, metrics_list, after_column='attractor_ratio')\n",
    "df1 = df[df[\"trajectory_length\"]>=10] # dataset1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "## Goal\n",
    "The goal of this study is to investigate the relation between **sampling strategy** and **model selection metrics** in *boolean Bayesian networks (BDNs)* using `BNFinder2`. \n",
    "\n",
    "In particular, we analyze how characteristics of time-series data generated from Boolean networks influence the accuracy of reconstructing the underlying network structure using Bayesian methods. The experimental factors under investigation include:\n",
    "- the **trajectory length**,\n",
    "- the **sampling frequency**, defined as selecting every $n$-th state along a trajectory,\n",
    "- the ratio between the **number of nodes** and the **trajectory length**, introduced as a normalization parameter $k$.\n",
    "\n",
    "The generated datasets are grouped into classes determined by:\n",
    "- the **update mode** (synchronous vs asynchronous),\n",
    "- the **scoring function** used during inference (MDL and BDe).\n",
    "\n",
    "In addition, we study **scaling relations with respect to the number of nodes**, aiming to characterize how data requirements and reconstruction accuracy change as network size increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Experimental design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Update Mode\n",
    "We distinguish between two fundamentally different update mechanisms:\n",
    "- **Synchronous update**, which defines a deterministic dynamical system: from any given state, the successor state is uniquely determined.\n",
    "- **Asynchronous update**, which induces a stochastic process: at each time step, a randomly selected node is updated, leading to multiple possible successor states.\n",
    "\n",
    "This distinction is critical, as asynchronous dynamics introduce temporal dependence and potential autocorrelation in trajectories. In particular, long residence times in attractors or local cycles may reduce the effective information content of sampled data. Consequently, naive dense sampling may lead to strongly correlated observations, while aggressive subsampling may destroy causal ordering information.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### Scoring Functions\n",
    "We employ two scoring functions implemented in BNFinder2, which differ in how they trade off data fit against model complexity.\n",
    "Let $G$ denote a candidate network structure and $D$ the observed dataset.\n",
    "\n",
    "**Minimal Description Length (MDL)**\n",
    "\n",
    "The MDL score is defined as\n",
    "$$\n",
    "\\mathrm{MDL}(G \\mid D)\n",
    "= - \\log P(D \\mid G, \\hat{\\theta})\n",
    "\\times \\frac{1}{2} , |\\theta_G| , \\log |D|,\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\hat{\\theta}$ are maximum-likelihood parameters and $|\\theta_G|$ denotes the number of free parameters implied by the graph structure $G$.\n",
    "\n",
    "The first term rewards goodness of fit, while the second term penalizes model complexity. As a consequence, MDL favors simpler graphs when data are scarce and becomes less restrictive as sample size increases. This makes MDL particularly sensitive to undersampling and normalization effects.\n",
    "\n",
    "**Bayesian–Dirichlet equivalence (BDe)**\n",
    "\n",
    "The BDe score evaluates the marginal likelihood\n",
    "\n",
    "$$\n",
    "\\mathrm{BDe}(G \\mid D)\n",
    "= - \\log \\int P(D \\mid G, \\theta), P(\\theta \\mid G), d\\theta,\n",
    "$$\n",
    "\n",
    "assuming a Dirichlet prior over conditional probability tables. Under standard assumptions, this integral has a closed-form.\n",
    "\n",
    "Unlike MDL, BDe incorporates prior beliefs and smooths parameter estimates, which can stabilize inference in low-data regimes but may also reduce sensitivity to subtle structural differences.\n",
    "\n",
    "By comparing MDL and BDe, we assess whether observed reconstruction effects are driven primarily by data properties or by the inductive bias of the scoring function.\n",
    "\n",
    "One caveat is that our implementation of those functions is simplified compared to implemented in BNfinder, value obtained is different but monotonicity is preserved and therefore do not affect the validity of our conclusions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics\n",
    "Reconstruction quality is evaluated using structure-based graph distance measures. Each metric captures a distinct notion of discrepancy between the true network $G^\\ast$ and the inferred network $\\hat{G}$. \n",
    "\n",
    "**Adjusted Hamming Distance (AHD)**\n",
    "\n",
    "Let $A^\\ast$ and $\\hat{A}$ denote the adjacency matrices of $G^\\ast$ and $\\hat{G}$. The adjusted Hamming distance is defined as\n",
    "\n",
    "$$\n",
    "\\mathrm{AHD}(G^\\ast, \\hat{G})\n",
    "= \\frac{1}{|E^\\ast| + |\\hat{E}|}\n",
    "\\sum_{i,j} \\mathbf{1}_{{A^\\ast_{ij} \\neq \\hat{A}_{ij}}}.\n",
    "$$\n",
    "AHD measures the proportion of mismatched edges, normalized by graph size and allows comparisons across networks of different sizes. This metric serves as the primary measure of structural accuracy.\n",
    "\n",
    "**Structural Hamming Distance (SHD)**\n",
    "\n",
    "SHD counts the minimum number of edge insertions, deletions, or reversals required to transform $\\hat{G}$ into $G^\\ast$.\n",
    "$\n",
    "\\mathrm{SHD}(G^\\ast, \\hat{G}) \\in \\mathbb{N}.\n",
    "$\n",
    "While widely used, SHD aggregates heterogeneous error types and does not distinguish between missing, extra, or misoriented edges, limiting its interpretability.\n",
    "\n",
    "**Structural Intervention Distance (SID)**\n",
    "\n",
    "SID measures the number of node pairs $(i,j)$ for which the causal effect of intervening on $i$ differs between $G^\\ast$ and $\\hat{G}$.\n",
    "\n",
    "$$\n",
    "\\mathrm{SID}(G^\\ast, \\hat{G})\n",
    "= \\left| {(i,j) : P(j \\mid \\mathrm{do}(i))*{G^\\ast}\n",
    "\\neq P(j \\mid \\mathrm{do}(i))*{\\hat{G}} } \\right|.\n",
    "$$\n",
    "Where $\\mathrm{do}(i=n)$ set node $i$ to have value $n$ at time step, regardles of Boolean update rule.\n",
    "\n",
    "SID is sensitive to edge orientation and captures errors that affect causal interpretability, even when the undirected structure is largely correct.\n",
    "\n",
    "Using these metrics jointly allows us to separate purely topological accuracy (AHD, SHD) from correctness of implied causal relationships (SID) and to identify metric-specific effects of sampling and model selection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Independence\n",
    "To avoid introducing structural bias, Boolean networks are generated randomly for each condition:\n",
    "- each node is assigned a random number of parents (uniformly chosen from $\\{ 1 ,2 ,3 \\}$),\n",
    "- Boolean update functions are sampled randomly.\n",
    "All generated networks are accepted without filtering. Repetitions under identical conditions are treated as independent realizations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Sample Size Normalization\n",
    "To ensure comparability across networks of different sizes, we introduce **sample-size normalization**.\n",
    "Let:\n",
    "- $n_{\\text{nodes}}$​ denote the number of nodes,\n",
    "- $T_{\\text{length}}$​ the trajectory length,\n",
    "- $k$ a normalization constant.\n",
    "The total number of sampled time points is defined as:\n",
    "$$\n",
    "\\begin{align}\n",
    "T_{\\text{amount}} & = \\frac{{n_{\\text{nodes}} \\cdot k}}{T_{\\text{length}}} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We fix $k=100$. This choice is motivated by the fact that each node with at most three parents has up to $2^3 = 8$ possible parent-state configurations. Setting $k=100$ corresponds to approximately 10 observations per configuration per node, providing a conservative buffer against stochastic effects and subsampling losses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Dataset Construction\n",
    "\n",
    "#### General Settings\n",
    "Across all experiments, we vary the following parameters:\n",
    "* number of nodes (`num_nodes`): ([5, 7, 11, 13, 15]),\n",
    "* scoring function (`score_function`): MDL, BDe,\n",
    "* update mode (`update_mode`): synchronous, asynchronous,\n",
    "* parent count per node: randomly chosen from ({1,2,3}),\n",
    "* number of repetitions per condition: 30.\n",
    "---\n",
    "#### Dataset 1 (Baseline Dataset)\n",
    "This dataset is used in Experiments 1 and 2.\n",
    "* trajectory length (`trajectory_length`): ([10, 15, 20, 30, 40, 50]),\n",
    "* sampling frequency (`sampling_frequency`): ([1, 2, 3, 4, 5]),\n",
    "* number of trajectories: determined via sample-size normalization.\n",
    "---\n",
    "#### Dataset 1.1 (Low-Data Regime)\n",
    "To probe behavior in extreme data-scarce settings, an auxiliary dataset is constructed with:\n",
    "* trajectory length: ([5, 7, 9]),\n",
    "* number of nodes: ([5, 7, 9]),\n",
    "* sampling frequency: ([1, 2, 3, 4, 5]).\n",
    "This dataset is included to assess scaling behavior when normalization constraints are tight.\n",
    "---\n",
    "#### Dataset 2 (Normalization Study)\n",
    "Using optimal parameters identified in Experiments 1 and 2, we fix:\n",
    "\n",
    "$$\\begin{align*} \\text{sampling frequency} & =\\begin{cases}1  & \\text{synchronous} \\\\ 2 & \\text{asynchronous and MDL} \\\\ 3 & \\text{asynchronous and BDE}\\end{cases} & \\mathrm{trajectory\\ length}=[0.8 \\cdot x]_{\\mathrm{round}}\\end{align*}$$\n",
    "\n",
    "We then vary the normalization constant:\n",
    "$$k \\in \\{20, 40, 60, 80, 100, 120, 140\\}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Experiments \n",
    "#### Experiment 1: Attractor Prevalence and Trajectory Length\n",
    "Motivated by prior observations that attractor-heavy datasets degrade reconstruction quality, we investigate:\n",
    "1. the correlation between attractor fraction and reconstruction metrics,\n",
    "2. how attractor prevalence depends on trajectory length,\n",
    "3. how these effects scale with network size.\n",
    "We introduce the **scale ratio**\n",
    "$$\\text{scale ratio} = \\frac{T_{\\text{length}}}{n_{\\text{nodes}}}$$\n",
    "and analyze its relationship with attractor fraction to identify regimes that balance coverage of transient dynamics and attractor exploration.\n",
    "\n",
    "---\n",
    "#### Experiment 2: Subsampling and Temporal Dependence\n",
    "The goal of this experiment is to identify an appropriate subsampling parameter by:\n",
    "\n",
    "0. Reducing the effective amount of data via subsampling.\n",
    "1. Examining a possible relationship between *data characteristics*, quantified using **lag-1 autocorrelation (ACF)** and **effective sample size (ESS)**.\n",
    "2. Examining a possible relationship between data characteristics and reconstruction quality.\n",
    "3. Assessing statistical significance of the relationship between data characteristics and reconstruction quality by evaluating **Spearman correlation tests** between ESS / ACF and reconstruction metrics, within groups defined by  \n",
    "   (`num_nodes`, `update_mode`, `score_function`, `sampling_frequency`).\n",
    "4. Assessing statistical significance of the relationship between **sampling frequency** and **reconstruction quality** by evaluating **Wilcoxon tests** on **AHD** and **SID** metrics for consecutive subsampling levels $(i \\rightarrow i+1)$, within groups defined by   \n",
    "   (`num_nodes`, `update_mode`, `score_function`).\n",
    "\n",
    "This experiment concludes with the identification of optimal subsampling parameters as a function of `update_mode` and `score_function`.\n",
    "\n",
    "---\n",
    "#### Experiment 3: Normalization and Information Saturation\n",
    "\n",
    "TODO - tutaj trzeba  poprawić po zrobieniu\n",
    "\n",
    "Fixing sampling parameters based on earlier results, we analyze how reconstruction metrics evolve as a function of the normalization constant ($k$). This allows us to identify saturation regimes in which increasing sample size yields diminishing returns.\n",
    "\n",
    "---\n",
    "#### Experiment 4: Score Function Sensitivity\n",
    "TODO - tutaj trzeba zmienić po zrobieniu\n",
    "Finally, using a subset of Experiment 3 with fixed normalization, we investigate how reconstruction metrics depend on the choice of scoring function. The goal is to understand whether differences in reconstruction quality arise from data properties or from intrinsic characteristics of the scoring criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Experiment 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Determinants of Reconstruction Accuracy\n",
    "\n",
    "The primary objective of this analysis was to identify the key factors influencing the accuracy of Boolean network reconstruction using dynamic Bayesian networks (DBNs). Specifically, we investigated how properties of the generated data (attractor ratio, trajectory length), network characteristics (number of nodes), and inference configuration (update mode and scoring function) affect reconstruction quality as measured by standard performance metrics.\n",
    "\n",
    "##### Exploratory Correlation Analysis\n",
    "\n",
    "We first conducted a pairwise correlation analysis between data characteristics and reconstruction metrics (precision, recall, SHD, and AHD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "corr_matrix = df1[['attractor_ratio', 'num_nodes', 'trajectory_length', 'precision', 'recall', 'SHD', 'AHD']].corr(method='spearman')\n",
    "g = sns.clustermap(corr_matrix, \n",
    "                   annot=True, \n",
    "                   cmap=\"coolwarm\",\n",
    "                   fmt=\".2f\",\n",
    "                   figsize=(10, 10),\n",
    "                   row_cluster=True,\n",
    "                   col_cluster=True,\n",
    "                   center=0,\n",
    "                   dendrogram_ratio=0.1,\n",
    "                   cbar_pos=(0.02, 0.8, 0.03, 0.18))\n",
    "g.ax_heatmap.set_title(\"Clustered Correlation Matrix\", fontsize=16, pad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "This exploratory step revealed several notable relationships:\n",
    "\n",
    "- **Attractor ratio** exhibited a strong negative correlation with recall (r = –0.48), indicating that datasets dominated by attractor states substantially impair the recovery of true regulatory interactions.\n",
    "- **Network size (number of nodes)** showed a strong positive correlation with SHD (r = 0.71), confirming that reconstruction error increases with network complexity.\n",
    "- **Trajectory length** displayed near-zero correlations with all performance metrics, suggesting that its effect is not uniform and likely depends on interactions with other factors.\n",
    "\n",
    "---\n",
    "\n",
    "#### Multivariate Regression Analysis\n",
    "\n",
    "To quantify the independent contribution of each factor while controlling for others, we fitted ordinary least squares (OLS) regression models for structural error (SHD) and adjacency error (AHD). The primary model for SHD included attractor ratio, trajectory length, number of nodes, update mode, scoring function, and an interaction between attractor ratio and trajectory length:\n",
    "\n",
    "```\n",
    "SHD ~ attractor_ratio × trajectory_length + num_nodes + update_mode + score_function\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\n",
    "    \"SHD ~ attractor_ratio * trajectory_length + num_nodes + update_mode + score_function\",\n",
    "    data=df1\n",
    ").fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "The model explained approximately 44% of the variance in SHD (R² ≈ 0.44), indicating substantial explanatory power. The following effects were statistically significant:\n",
    "\n",
    "- **Attractor ratio** had a strong positive effect on SHD, demonstrating that a higher proportion of attractor states in the data leads to increased structural reconstruction error.\n",
    "- **Update mode** (synchronous vs. asynchronous) exerted a large effect, with synchronous updates yielding significantly higher error. This result was expected, as synchronous dynamics reduce the diversity of observable transitions, effectively decreasing the informational content of the data.\n",
    "- **Scoring function** had a strong effect, with MDL outperforming BDe in terms of lower structural error.\n",
    "- **Trajectory length** had a smaller but statistically significant effect on SHD, indicating that longer trajectories do not necessarily improve reconstruction accuracy under fixed sample size conditions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Interaction Effects and Mechanistic Insight\n",
    "\n",
    "To further investigate how data characteristics interact, we fitted a second regression model for adjacency error (AHD) including the interaction term:\n",
    "\n",
    "```\n",
    "AHD ~ attractor_ratio × trajectory_length + num_nodes + update_mode + score_function\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = smf.ols(\n",
    "    \"AHD ~ attractor_ratio * trajectory_length + num_nodes + update_mode + score_function\",\n",
    "    data=df1\n",
    ").fit()\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "This model revealed a significant negative interaction between attractor ratio and trajectory length, indicating that longer trajectories partially mitigate the detrimental effect of high attractor dominance. In other words, while attractor-heavy datasets are generally less informative, increasing trajectory length can partially compensate by providing more opportunities to observe transient dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary of Determinants\n",
    "\n",
    "Overall, reconstruction accuracy was primarily influenced by:\n",
    "\n",
    "1. **Update mode**, which strongly affected performance due to differences in the stochasticity and informational richness of the generated trajectories. This effect is intuitive, as reduced dynamical variability limits the ability to infer causal relationships.\n",
    "2. **Scoring function**, with MDL consistently outperforming BDe under the studied conditions.\n",
    "3. **Attractor ratio**, which emerged as the most influential data-related factor, particularly impairing recall and increasing structural error.\n",
    "4. **Network size**, which substantially increased structural error due to scaling complexity.\n",
    "5. **Trajectory length**, which had a secondary, interaction-dependent effect rather than a uniform influence.\n",
    "\n",
    "These findings demonstrate that reconstruction accuracy is jointly determined by data composition, network complexity, and inference configuration, with attractor dominance playing a central role in limiting the informativeness of time-series data.\n",
    "\n",
    "#### Visualizing the Impact of Attractor Ratio on Reconstruction Error\n",
    "\n",
    "To complement our regression analysis and gain an intuitive understanding of how **attractor dominance interacts with scoring function and update mode**, we generated scatter plots of **SHD, AHD, BDe and MDL versus attractor ratio**, stratified by **synchronous and asynchronous updates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_colors = ['#C0392B', '#2980B9', '#27AE60', '#F4D03F', '#8E44AD']\n",
    "\n",
    "plot_scatter_subplots(df1, x='attractor_ratio', y='SHD', title='SHD vs Attractor Ratio by Update Mode')\n",
    "plot_scatter_subplots(df1, x='attractor_ratio', y='AHD', title='AHD vs Attractor Ratio by Update Mode')\n",
    "plot_scatter_subplots(df1, x='attractor_ratio', y='BDe', title='BDE vs Attractor Ratio by Update Mode')\n",
    "plot_scatter_subplots(df1, x='attractor_ratio', y='MDL', title='MDL vs Attractor Ratio by Update Mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "##### Results:\n",
    "\n",
    "* In the **SHD plots**, under **synchronous updates**, most points are clustered at **high attractor ratios (>0.6)**, with SHD ranging broadly from **0 to 80**, reflecting the substantial structural error associated with attractor-dominated datasets. In contrast, **asynchronous updates** show attractor ratios spanning the full range, but SHD is generally much lower for low attractor ratio trajectories, increasing gradually up to **~40**, confirming that asynchronous dynamics produce more informative trajectories.\n",
    "* For **AHD**, synchronous updates again concentrate at **attractor ratios >0.6**, with AHD ranging between **0.1–0.8**, whereas asynchronous updates cover the full attractor range but exhibit a dense cluster near **0–0.5**, indicating overall lower adjacency errors compared to synchronous dynamics.\n",
    "* For **BDe**, synchronous updates concentrate mostly at **attractor ratios >0.6, with BDe ranging between 0.0-2500**. In asynchronous updates, however, a downward trend in BDe can be observed for the atraktor_ratio range of 0.0-0.4. After that, most of the values ​​accumulate close to zero.\n",
    "* For **MDE**, in synchronous updates attractor ratio were >0.4 and values of MDL were ranges between 0-50000, for asynchronous attractor ratio were spanning the full range and MDL were mostly ranging between 0.0-100000.\n",
    "\n",
    "Together, these scatter plots visually confirm our earlier findings: **high attractor ratios are associated with greater reconstruction errors**, and the **update mode strongly shapes the distribution of errors**, with asynchronous updates producing generally more accurate reconstructions. These visualizations provide a bridge to investigating **which factors influence attractor prevalence**, a key driver of reconstruction difficulty.\n",
    "\n",
    "**This observation motivates the next step of our study: since attractor ratio is a key driver of reconstruction error, we now investigate which network- and data-generating factors determine the prevalence of attractor states in Boolean network trajectories.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "##### 1. Relation Between Trajectory Length and Entering Attractors\n",
    "\n",
    "**Objective.**\n",
    "To characterize how trajectory length is related to the probability of entering attractors as a function of network size and dynamics.\n",
    "\n",
    "**Experimental design.**\n",
    "\n",
    "- The target attractor proportion is **not controlled**; trajectories evolve naturally.\n",
    "- Trajectory lengths are varied in increments proportional to network size:\n",
    "    - from 10 steps to 50 by 10\n",
    "- Networks are grouped by size (from 4 to 16 nodes, in steps of two).\n",
    "- The number of parents per node is randomly chosen from set of $\\{1,2,3\\}$ to avoid conditioning results on a fixed connectivity pattern.\n",
    "\n",
    "**Measured quantities.**\n",
    "\n",
    "- Probability of reaching an attractor as a function of trajectory length.\n",
    "- How different groups (below TODO - inner link) differ in in this probability.\n",
    "\n",
    "**Rationale.**\n",
    "Attractor entry is an emergent property of the dynamics. Controlling it directly is undesirable, as it would introduce selection bias. This experiment instead characterizes the **natural scaling behavior** of Boolean network dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synchronous = df1[df1[\"update_mode\"]==\"synchronous\"]\n",
    "df_asynchronous = df1[df1[\"update_mode\"]==\"asynchronous\"]\n",
    "\n",
    "# Boxplot grouped by num_nodes synchronous\n",
    "plot_boxplot(df_synchronous, x='trajectory_length', y='attractor_ratio', hue='num_nodes',\n",
    "             title='Synchronous: Attractor Ratio vs Trajectory Length\\nGrouped by num_nodes')\n",
    "# Boxplot grouped by num_nodes synchronous\n",
    "plot_boxplot(df_asynchronous, x='trajectory_length', y='attractor_ratio', hue='num_nodes',\n",
    "             title='Asynchronous: Attractor Ratio vs Trajectory Length\\nGrouped by num_nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "To further explore the factors influencing attractor prevalence, we generated boxplots of **attractor ratio versus trajectory length**, grouped by **network size (num_nodes)**, separately for **synchronous** and **asynchronous** updates.\n",
    "\n",
    "* **Synchronous updates:** attractor ratios are generally higher across all trajectory lengths, consistent with the expected behavior of synchronous Boolean networks, where simultaneous node updates tend to drive the system more quickly into attractor states.\n",
    "* **Asynchronous updates:** attractor ratios are lower overall and more widely distributed, reflecting the increased variability and longer transient dynamics inherent to asynchronous update schemes.\n",
    "\n",
    "Across both update modes, we observe consistent trends:\n",
    "\n",
    "1. **Trajectory length effect:** attractor ratio tends to increase with longer trajectories.\n",
    "2. **Network size effect:** attractor ratio tends to decrease as the number of nodes increases.\n",
    "\n",
    "These observations visually confirm the patterns suggested by our regression and scatter plot analyses, providing an intuitive view of how **update mode, trajectory length, and network size jointly shape attractor prevalence** in simulated Boolean networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "#### Scaling Law Analysis: Normalizing Trajectory Length by Network Size\n",
    "\n",
    "To systematically compare how trajectory length and network size jointly influence convergence, we introduced a **scaling metric**: the **scale_ratio**, defined as `trajectory_length / num_nodes`. This metric normalizes the simulation time by the network’s complexity, allowing us to identify a unified scaling behavior across different system sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = (\n",
    "    df\n",
    "    .groupby([\"trajectory_length\", \"num_nodes\"])\n",
    "    .agg(\n",
    "        median_ar=(\"attractor_ratio\", \"median\"),\n",
    "        q25_ar=(\"attractor_ratio\", lambda x: x.quantile(0.25)),\n",
    "        mean_ar=(\"attractor_ratio\", \"mean\"),\n",
    "        std_ar=(\"attractor_ratio\", \"std\"),\n",
    "        n=(\"attractor_ratio\", \"size\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_summary[\"scale_ratio\"] = (\n",
    "    df_summary[\"trajectory_length\"] / df_summary[\"num_nodes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df_summary,\n",
    "    x=\"scale_ratio\",\n",
    "    y=\"median_ar\",\n",
    "    hue=\"num_nodes\",\n",
    "    palette=\"tab10\",\n",
    "    s=80,       \n",
    "    alpha=0.7   \n",
    ")\n",
    "\n",
    "plt.xlabel(\"Scale Ratio (trajectory_length / num_nodes)\")\n",
    "plt.ylabel(\"Median Attractor Ratio\")\n",
    "plt.title(\"Median Attractor Ratio vs Scale Ratio\", fontsize=16)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.legend(title=\"Num Nodes\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "##### **Results:**\n",
    "We could notice that **the lower scale ratio, the lower median attractor ratio**. This means that there is a significant relationship between the **structure of the Boolean network** on the basis of which we create our trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust data\n",
    "## Add results from part 1\n",
    "### Create new df for reproducibility \n",
    "df_series = df.copy()\n",
    "### dropping values < threshold\n",
    "df_series['scale'] = df_series[\"trajectory_length\"] / df_series[\"num_nodes\"]\n",
    "df_series = df_series[df_series['scale'] < 1.5]\n",
    "### dropping sets that are meaningles\n",
    "df_series = df_series[\n",
    "    (df_series[\"trajectory_length\"] / df_series[\"sampling_frequency\"]) > 1\n",
    "]\n",
    "## Add ACF and ESS information \n",
    "df_series = analyze_datasets_from_index(df_series, 'condition_id_name', DATASET_1_PATH, 1)\n",
    "\n",
    "# merge \n",
    "df_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "#### Part 0: Sample Size Adequacy and Statistical Validity \n",
    "\n",
    "Before applying any statistical tests, we verify that the number of available observations remains sufficient after filtering the data. In particular, we removed:\n",
    "\n",
    "* configurations with scale ratio below 1.5,\n",
    "* cases without effective subsampling (single-sample trajectories).\n",
    "\n",
    "After this filtering step, we assess whether the remaining sample sizes are still representative.\n",
    "\n",
    "The resulting sample counts are as follows:\n",
    "\n",
    "* **sampling frequency × number of nodes**: at least 240 observations per group, and up to 480 in larger configurations,\n",
    "* **scoring function × number of nodes**: between 540 and 1140 observations, depending on the condition.\n",
    "\n",
    "These sample sizes are quite large, and still have all repetitions for every experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling frequency vs num_nodes\n",
    "table = pd.pivot_table(\n",
    "    df_series,\n",
    "    index='sampling_frequency',\n",
    "    columns='num_nodes',\n",
    "    aggfunc='size',\n",
    "    fill_value=0\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score function vs num_nodes\n",
    "table = pd.pivot_table(\n",
    "    df_series,\n",
    "    index='score_function',\n",
    "    columns='num_nodes',\n",
    "    aggfunc='size',\n",
    "    fill_value=0\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### Part 1: ESS and ACF vs Sampling Frequency \n",
    "\n",
    "##### Methodology \n",
    "\n",
    "We analyze the **mean lag-1 autocorrelation**, denoted `mean_lag1_acf`, which measures dependence between consecutive sampled time points. Lag 1 is chosen because it captures correlation between two adjacent time points.  \n",
    "\n",
    "ESS is computed to quantify the amount of effectively independent information contained in the sampled trajectories. Higher ESS should bring more information to model training and improve reconstruction quality. \n",
    "\n",
    "---\n",
    "\n",
    "##### Results: Lag-1 Autocorrelation (ACF)\n",
    "\n",
    "For **synchronous update**, the distribution of `mean_lag1_acf` is centered close to zero across all sampling frequencies and network sizes. No systematic trend with respect to sampling frequency is observed. This behavior is expected, as synchronous dynamics define a deterministic mapping between states; subsampling does not substantially alter short-range temporal dependence.\n",
    "\n",
    "For **asynchronous update**, we observe:\n",
    "\n",
    "* a clear decrease in lag-1 autocorrelation when increasing sampling frequency from 1 to 3,\n",
    "* a weaker but still noticeable change between frequencies 1 and 4,\n",
    "* a potential degradation or saturation between frequencies 4 and 5.\n",
    "\n",
    "These observations suggest that subsampling reduces short-range dependence in asynchronous trajectories up to a point, beyond which additional subsampling start to increase ACF. The non-monotonic behavior at higher sampling frequencies requires further statistical testing to assess significance.\n",
    "\n",
    "---\n",
    "\n",
    "##### Results: Effective Sample Size (ESS) \n",
    "\n",
    "For **synchronous update**, ESS increases clearly with sampling frequency across all network sizes. This indicates that subsampling effectively reduces redundancy in deterministic dynamics. \n",
    "\n",
    "For **asynchronous update**, the pattern is less clear:\n",
    "\n",
    "* ESS values are substantially lower than in the synchronous case,\n",
    "* increases in sampling frequency do not translate into uniform gains in ESS,\n",
    "* ESS appears to depend jointly on sampling frequency and network size.\n",
    "\n",
    "This suggests that stochastic update dynamics introduce long-range temporal correlations that are not fully mitigated by simple subsampling. As a result, sampling frequency alone is insufficient to ensure an increase in ESS in asynchronous trajectories.\n",
    "\n",
    "##### Conclusion\n",
    "For both **synchronous** and **asynchronous** update there is no clear relation between ESS and ACF. In next parts we will need to examine them independently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_grouped_boxplots(\n",
    "    df=df_series,\n",
    "    group_col=None,                       # ← kluczowe\n",
    "    x_col=\"sampling_frequency\",\n",
    "    y_cols=[\"mean_lag1_acf\", \"mean_ess\"],\n",
    "    hue_col=\"num_nodes\",\n",
    "    facet_col=\"update_mode\",              # ← porównanie sync vs async\n",
    "    facet_levels=[\"synchronous\", \"asynchronous\"],\n",
    "    main_title=\"Mixing diagnostics vs sampling frequency\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Analysis 2: Reconstruction quality vs sampling frequency\n",
    "\n",
    "##### Methodology\n",
    "\n",
    "In this analysis, we study the relationship between **sampling frequency** and **model reconstruction quality**, measured using the structural accuracy metrics **AHD** and **SID**.\n",
    "\n",
    "The objective is to determine how changes in sampling frequency affect the quality of inferred network structures.\n",
    "\n",
    "---\n",
    "\n",
    "##### Analysis: Synchronous update \n",
    "\n",
    "For the **synchronous update mode**, a clear pattern is observed in both reconstruction metrics.\n",
    "\n",
    "As shown in:\n",
    "\n",
    "* **AHD vs sampling frequency** plots (top row, synchronous panels),\n",
    "* **SID vs sampling frequency** plots (middle upper row, synchronous panels),\n",
    "\n",
    "the smallest values of both AHD and SID are obtained for **sampling frequency = 1**, across all network sizes and for both MDL and BDe scoring functions. Increasing the sampling frequency leads to a systematic degradation of reconstruction quality.\n",
    "\n",
    "Given the consistency of this effect across metrics, scoring functions, and network sizes, no further statistical testing is required for the synchronous case.\n",
    "\n",
    "##### Analysis: Asynchronous update\n",
    "\n",
    "For the **asynchronous update mode**, the behavior is not as clear.\n",
    "\n",
    "From the corresponding plots:\n",
    "\n",
    "* **AHD vs sampling frequency** (middle lower row, asynchronous panels),\n",
    "* **SID vs sampling frequency** (bottom row, asynchronous panels),\n",
    "\n",
    "we observe that reconstruction quality generally improves when increasing sampling frequency from 1 to approximately 3. Beyond this point improvement becomes inconsistent across network sizes and score functions.\n",
    "\n",
    "This lack of monotonicity indicates that, in the asynchronous case, the relationship between sampling frequency and reconstruction quality is mediated by additional factors, such as long-range temporal dependence and network size. Consequently, inferential statistical tests are required to assess the significance of these trends.\n",
    "\n",
    "---\n",
    "\n",
    "##### Conclusions\n",
    "\n",
    "We find a clear relationship between **sampling frequency** and **model reconstruction quality**, but this relationship is different for synchronous and asynchronous case:\n",
    "\n",
    "* For **synchronous dynamics**, the optimal choice is **sampling frequency = 1**, as any subsampling leads to a consistent loss of structural and causal accuracy.\n",
    "* For **asynchronous dynamics**, reconstruction quality generally improves with subsampling up to a moderate frequency (around 3), but no single optimal value can be identified without further statistical testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grouped_boxplots(\n",
    "    df=df_series,\n",
    "    group_col=\"update_mode\",\n",
    "    x_col=\"sampling_frequency\",\n",
    "    y_cols=[\"AHD\", \"SID\"],\n",
    "    hue_col=\"num_nodes\",\n",
    "    facet_col=\"score_function\",\n",
    "    facet_levels=[\"MDL\", \"BDE\"],\n",
    "    main_title=\"Model performance vs sampling frequency\",\n",
    "    group_title_fmt=\"Update mode = {}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "\n",
    "#### Part 3 – Spearman Correlation Analysis\n",
    "\n",
    "##### Methodology\n",
    "\n",
    "The main objective of this analysis is to assess whether properties of a data: **ESS** and **lag-1 ACF**, are associated with **model reconstruction quality**, measured using AHD and SID.\n",
    "\n",
    "we compute **Spearman rank correlations** between:\n",
    "\n",
    "* ESS and reconstruction metrics,\n",
    "* lag-1 ACF and reconstruction metrics,\n",
    "\n",
    "separately for groups defined by:\n",
    "\n",
    "* update mode,\n",
    "* score function,\n",
    "* number of nodes.\n",
    "* sampling_frequency\n",
    "\n",
    "After prior filtering, each group contains between **60 and 120 observations**, which is sufficient for stable estimation of Spearman correlations if $\\rho \\geq 0.4$ (see Bonett & Wright, 2000).  Correlations are presented as heatmaps, independently for each combination of `num_nodes` and `score_function` and `update_mode`.\n",
    "\n",
    "This analysis is conducted separately for ESS and ACF, as results from part 1 indicate that these quantities are not interchangeable and may capture different aspects of temporal dependence.\n",
    "\n",
    "---\n",
    "\n",
    "##### Results: ESS\n",
    "\n",
    "The results for ESS are summarized in the **ESS–AHD** and **ESS–SID** heatmaps.\n",
    "\n",
    "For the **synchronous update mode** (top heatmaps), Spearman correlation coefficients are close to zero across all network sizes and both scoring functions. No consistent monotonic relationship between ESS and reconstruction quality is observed for either AHD or SID.\n",
    "\n",
    "In contrast, for the **asynchronous update mode** (bottom heatmaps), a clear negative trend is visible:\n",
    "\n",
    "* ESS exhibits moderate negative correlations with AHD and SID,\n",
    "* correlations are stronger for larger networks, ($\\geq 9$)\n",
    "* several correlations (3/12) are statistically significant.\n",
    "\n",
    "This may suggest that, in asynchronous systems, higher effective sample size is associated with improved reconstruction quality, whereas in synchronous systems ESS does not appear to be a limiting factor. \n",
    "\n",
    "---\n",
    "\n",
    "##### Results: Lag-1 Autocorrelation (ACF)\n",
    "\n",
    "The analysis of lag-1 ACF reveals a complementary pattern.\n",
    "\n",
    "For **synchronous update**, correlations between ACF and reconstruction metrics remain weak and inconsistent across all configurations, confirming that short-range autocorrelation does not meaningfully affect reconstruction quality in this regime.\n",
    "\n",
    "For **asynchronous update**, we observe statistically significant correlations in selected configurations with larger BN (9, 11, 13):\n",
    "\n",
    "* a significant correlation between ACF and **AHD** for the **MDL** score,\n",
    "* a significant correlation between ACF and **SID** for the **BDe** score.\n",
    "\n",
    "These effects are visible in the asynchronous ACF heatmaps and suggest that \n",
    "dependence on neighborhood samples influence negatively reconstruction quality.\n",
    "\n",
    "---\n",
    "\n",
    "##### Conclusions\n",
    "\n",
    "We conclude that:\n",
    "\n",
    "* There is **no meaningful monotonic relationship** between ESS or ACF and reconstruction quality in synchronous systems.\n",
    "* In asynchronous systems, **higher ESS is may be associated with improved reconstruction quality**.\n",
    "* In asynchronous systems, **Lag-1 ACF influences negatively reconstruction quality**\n",
    "\n",
    "##### Reference\n",
    "\n",
    "Bonett, D. G., & Wright, T. A. (2000). Sample size requirements for estimating Pearson, Kendall and Spearman correlations. Psychometrika, 65, 23–28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"update_mode\", \"score_function\", \"num_nodes\", \"sampling_frequency\"]\n",
    "\n",
    "df_spearman = compute_spearman_table(\n",
    "    df_series,\n",
    "    metrics=[\"AHD\", \"SID\"],\n",
    "    group_cols = group_cols,\n",
    "    ess_col= \"mean_ess\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for update_mode in [\"synchronous\", \"asynchronous\"]:\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        1, 2,\n",
    "        figsize=(14, 5),\n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    plot_spearman_heatmap(\n",
    "        df_spearman,\n",
    "        metric=\"AHD\",\n",
    "        update_mode=update_mode,\n",
    "        num_nodes_order=sorted(df_series[\"num_nodes\"].unique(), reverse=True),\n",
    "        ax=axes[0],\n",
    "        cbar=False\n",
    "    )\n",
    "\n",
    "    plot_spearman_heatmap(\n",
    "        df_spearman,\n",
    "        metric=\"SID\",\n",
    "        update_mode=update_mode,\n",
    "        num_nodes_order=sorted(df_series[\"num_nodes\"].unique(), reverse=True),\n",
    "        ax=axes[1],\n",
    "        cbar=True\n",
    "    )\n",
    "\n",
    "    fig.suptitle(\n",
    "        \"Spearman correlation between ESS and reconstruction quality\\n\"\n",
    "        f\"Update mode = {update_mode}\",\n",
    "        fontsize=18,\n",
    "        y=1.05\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"update_mode\", \"score_function\", \"num_nodes\", \"sampling_frequency\"]\n",
    "\n",
    "df_spearman = compute_spearman_table(\n",
    "    df_series,\n",
    "    metrics=[\"AHD\", \"SID\"],\n",
    "    group_cols = group_cols,\n",
    "    ess_col= \"mean_lag1_acf\"\n",
    ")\n",
    "\n",
    "\n",
    "counts = (\n",
    "    df_series\n",
    "    .groupby(group_cols)\n",
    "    .size()\n",
    "    .reset_index(name=\"n_rows\")\n",
    ")\n",
    "counts \n",
    "\n",
    "for update_mode in [\"synchronous\", \"asynchronous\"]:\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        1, 2,\n",
    "        figsize=(14, 5),\n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    plot_spearman_heatmap(\n",
    "        df_spearman,\n",
    "        metric=\"AHD\",\n",
    "        update_mode=update_mode,\n",
    "        num_nodes_order=sorted(df_series[\"num_nodes\"].unique(), reverse=True),\n",
    "        ax=axes[0],\n",
    "        cbar=False\n",
    "    )\n",
    "\n",
    "    plot_spearman_heatmap(\n",
    "        df_spearman,\n",
    "        metric=\"SID\",\n",
    "        update_mode=update_mode,\n",
    "        num_nodes_order=sorted(df_series[\"num_nodes\"].unique(), reverse=True),\n",
    "        ax=axes[1],\n",
    "        cbar=True\n",
    "    )\n",
    "\n",
    "    fig.suptitle(\n",
    "        \"Spearman correlation between ACF and reconstruction quality\\n\"\n",
    "        f\"Update mode = {update_mode}\",\n",
    "        fontsize=18,\n",
    "        y=1.05\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = (\n",
    "    df_series\n",
    "    .groupby(group_cols)\n",
    "    .size()\n",
    "    .reset_index(name=\"n_rows\")\n",
    ")\n",
    "counts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Part 4. Does increasing sampling frequency significantly improve median reconstruction quality?\n",
    "\n",
    "##### Methodology\n",
    "\n",
    "The objective of this analysis is to assess whether **increasing the sampling frequency (sampling factor)** leads to **statistically significant changes in the median values of reconstruction quality metrics** for AHD and SID.\n",
    "\n",
    "The analysis is conducted **exclusively for the asynchronous update mode**, as in the synchronous setting the descriptive results clearly indicate that the best reconstruction quality is achieved for **sampling factor equal to 1**, making further statistical comparisons unnecessary.\n",
    "\n",
    "To isolate the effect of sampling frequency, all statistical tests are performed **within homogeneous experimental groups**, defined by the following variables:\n",
    "\n",
    "* update mode,\n",
    "* score function,\n",
    "* number of nodes.\n",
    "\n",
    "For each group, **paired comparisons** are performed between consecutive sampling factors (1→2, 2→3, 3→4, 4→5). The analysis considers two reconstruction quality metrics:\n",
    "\n",
    "* AHD,\n",
    "* SID,\n",
    "\n",
    "and two score functions:\n",
    "\n",
    "* BDe,\n",
    "* MDL.\n",
    "\n",
    "Statistical significance of median differences is assessed using the **single (greater) Wilcoxon signed-rank test**.\n",
    "\n",
    "Depending on the experimental condition, the number of paired observations equals **900, 1800, or 3600**, which allows the applied tests to be considered as having **high statistical power**.\n",
    "\n",
    "The results are summarized in a heatmap that presents:\n",
    "\n",
    "* the median difference between consecutive sampling factors,\n",
    "* statistical significance levels for each comparison.\n",
    "\n",
    "---\n",
    "\n",
    "##### Analysis\n",
    "\n",
    "For **BDe**, a **statistically significant improvement in reconstruction quality is observed for the transition from sampling factor 2 to 3**, indicating a meaningful change in the ranking of graph structures induced by this cost function. No further gains are observed for higher sampling factors.\n",
    "\n",
    "In contrast, when using **MDL**, statistically significant differences are observed only for the  transition from **sampling factor 1 to 2**. \n",
    "\n",
    "Importantly, these effects reflect changes in the **values of the cost functions used to rank candidate graph structures**, rather than direct improvements in the reconstruction metrics themselves. The observed differences may arise from BDe and MDL different construction, and usage of prior information from BDe.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##### Recommended configuration\n",
    "\n",
    "Based on the obtained results, the following recommendations can be formulated:\n",
    "\n",
    "* **synchronous**:\n",
    "  the optimal sampling factor is **3** (part 2)\n",
    "\n",
    "* **BDe (asynchronous)**:\n",
    "  the optimal sampling factor is **3**, we show that changes in direction (1-2-3) significantly increase reconstruction quality (AHD, SID). \n",
    "\n",
    "* **MDL (asynchronous)**:\n",
    "  the optimal sampling factor is **2**, we show that changes in direction (1-2) significantly increase reconstruction quality (AHD, SID). \n",
    "\n",
    "Higher sampling factors does not bring any differences, to reconstruction quality, it may only reduce them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"update_mode\", \"score_function\", \"num_nodes\"]\n",
    "df_wilcoxon = compute_wilcoxon_table(\n",
    "    df_series,\n",
    "    metrics=[\"AHD\", \"SID\"],\n",
    "    group_cols=group_cols,\n",
    "    transitions=[(1,2), (2,3), (3,4), (4,5)], #TODO - tutaj wstawic kolejnośc z jaką chcesz sprawdzać (20, 40) ...\n",
    "    sf_col='sampling_frequency' #TODO - tutaj wstawić tą kolumne co masz stałą normalizacji \n",
    ")\n",
    "\n",
    "df_wilcoxon['n_pairs']\n",
    "df_wilcoxon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "plot_wilcoxon_heatmap(\n",
    "    df_wilcoxon,\n",
    "    metric=\"AHD\",\n",
    "    update_mode=\"asynchronous\",\n",
    "    transitions_order=sorted(df_wilcoxon['transition'].unique(), reverse=True),\n",
    "    ax=axes[0],\n",
    "    cbar=False\n",
    ")\n",
    "\n",
    "plot_wilcoxon_heatmap(\n",
    "    df_wilcoxon,\n",
    "    metric=\"SID\",\n",
    "    update_mode=\"asynchronous\",\n",
    "    transitions_order=sorted(df_wilcoxon['transition'].unique(), reverse=True),\n",
    "    ax=axes[1],\n",
    "    cbar=True\n",
    ")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Paired Wilcoxon test: effect of increasing sampling frequency\\n\"\n",
    "    \"Asynchronous update\",\n",
    "    fontsize=18,\n",
    "    y=1.05\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Experiment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "#### Part 1: Distribution of metrics as function of normalization value $k$.\n",
    "\n",
    "##### Methodology \n",
    "\n",
    "We want to analyze if there is some relation between normalization value $k$ and reconstruction quality. \n",
    "\n",
    "We draw barplots of metrics (SHD, AHD, SID) as function of $k$. With respect to experimental groups. \n",
    "\n",
    "\n",
    "##### Analysis\n",
    "\n",
    "**Synchronous networks**\n",
    "\n",
    "In synchronous network we can observe that:\n",
    "* SHD have tendency to raise for higher $k$ values,\n",
    "* AHD and SID obtain the smallest IQR when $k$ value is between $40, 60$\n",
    "\n",
    "It can be caused because of overfitting the model. SHD rising value is expected because this measure is not normalized to the size of network, although AHD and SID metrics are getting worse at the same time so model is probably making more FP edges. \n",
    "\n",
    "\n",
    "**Asynchronous network**\n",
    "\n",
    "In asynchronous network we can observe that:\n",
    "* SHD value is not changing visibly, \n",
    "* AHD and SID values are getting lower with each $k$ value. \n",
    "\n",
    "The model probably need more samples to get fit. Asynchronous models have more edges than synchronous this can cause difference in optimal $k$ value.\n",
    "\n",
    "**Relation between $k$ value and boolean network size**\n",
    "\n",
    "We can see that amount of probes is not only dependent on size of network. Despite usage of normalization we see differences between distributions due to network size. We will\n",
    "\n",
    "**Differences between score functions**\n",
    "\n",
    "**Conclusions**\n",
    "- We can not assume that size of network is irrelevant (?):\n",
    "    - Despite normalization, size of network still influence reconstruction quality. We \n",
    "- We see differences in optimal $k$ value fro synchronous and asynchronous networks. Probably due to complexity differences between them. \n",
    "- We suspect that optimal values of $k$ is:\n",
    "    - around 40-60 for synchronous networks,\n",
    "    - around 120 for asynchronous networks.\n",
    "To make any significant conclusions, we need to perform statistical tests, because differences are to blurry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grouped_boxplots(\n",
    "    df=pd.concat([dfs, dfa], axis=0),\n",
    "    group_col=\"update_mode\",                       # ← kluczowe\n",
    "    x_col=\"k_value\",\n",
    "    y_cols=[\"SHD\", \"AHD\", \"SID\"],\n",
    "    hue_col=\"num_nodes\",\n",
    "    facet_col=\"score_function\",              # ← porównanie sync vs async\n",
    "    facet_levels=[\"MDL\", \"BDE\"], # \n",
    "    main_title=\"Reconstruction quality vs k value\",\n",
    "    group_title_fmt=\"Update mode = {}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "#### Part 2: Statistical analysis - optimal $k$ value\n",
    "\n",
    "##### Methodology\n",
    "\n",
    "We want to examine whether there exists significant relation between $k$ value and reconstruction quality. We use for it Wilcoxon tests (median difference). Group were specified by:\n",
    "- update mode\n",
    "- score function\n",
    "- size of network \n",
    "\n",
    "##### Analysis\n",
    "\n",
    "**Synchronous**\n",
    "In synchronous network we can observe that:\n",
    "- SHD median in most cases is significantly worse than in previous amount if iterations. We can see that error is scaling with size of network ($N=11$, $N=13$), this is due to SHD is not normalized metric.\n",
    "- AHD median is significantly worse oraz neutral.\n",
    "- SID median have two different pattern:\n",
    "    - networks of size $N=5, 7$ have mixed results, we can not see and pattern\n",
    "    - networks of size $N = 9, 11, 13$, have significant better results with $k=40$, \n",
    "\n",
    "\n",
    "**Asynchronous** \n",
    "In asynchronous network we can observe that:\n",
    "- SHD median have smaller value in more complicated networks which is counter intuitive, because SHD measure is not normalized\n",
    "- AHD median mostly is without any significant change that changes metric value. In case of network with size $N=5$, is significantly better\n",
    "- SID median is increasing significantly. Case ($N=5$ MDL $40\\rightarrow60$) can be significantly negative because of higher decrease in case ($N=5$, MDL, $20\\rightarrow60$)\n",
    "\n",
    "\n",
    "**Impact of network size**\n",
    "We do not observe clear impact of network size between test. In my opinion all results are shift by base margin error value that boolean networks of certain size can obtain.\n",
    "\n",
    "**Impact of score function**\n",
    "We do not observe any clear relation between tests results and score function used for training.\n",
    "\n",
    "##### Recommended configuration\n",
    "Based on above reasoning we suggest following $k$ value:\n",
    "\n",
    "| Network type / condition | k   |\n",
    "|--------------------------|-----|\n",
    "| Synchronous, N ≤ 5       | 20  |\n",
    "| Synchronous, N > 5       | 40  |\n",
    "| Asynchronous             | 100 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wilcoxon_s = compute_wilcoxon_table(\n",
    "    dfs,\n",
    "    metrics=[\"AHD\", \"SHD\", \"SID\"],\n",
    "    transitions=[(i*20, (i+1)*20) for i in range(1, 7)],\n",
    "    group_cols=('update_mode', 'score_function', 'num_nodes'),\n",
    "    sf_col='k_value',\n",
    "    alternative='two-sided'\n",
    ")\n",
    "df_wilcoxon_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "metrics= [\"SHD\", \"AHD\", \"SID\"]\n",
    "\n",
    "# values preparation\n",
    "## Creating transition order (special sign) \n",
    "transitions_order = sorted(df_wilcoxon_s['transition'].unique(), \n",
    "       key = lambda x: int(x.split('→')[0]),\n",
    "       reverse=True)\n",
    "## get list of nodes\n",
    "num_nodes_list = df_wilcoxon_s['num_nodes'].unique()\n",
    "\n",
    "# Metric adjustment loop\n",
    "for metric in metrics:\n",
    "    ## common scale\n",
    "    df_m = df_wilcoxon_s[df_wilcoxon_s[\"metric\"] == metric]\n",
    "    abs_max = np.nanmax(np.abs(df_m[\"median_diff\"]))\n",
    "    vmin, vmax = -abs_max, abs_max\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(14, 5), sharey=True)\n",
    "    ## Single heatmap loop\n",
    "    for i, num_nodes in enumerate(num_nodes_list):\n",
    "        ### filt node value \n",
    "        filt = df_wilcoxon_s['num_nodes'] == num_nodes\n",
    "        ### add num_nodes plot \n",
    "        plot_wilcoxon_heatmap(\n",
    "            #### values\n",
    "            df_wilcoxon_s[filt],\n",
    "            metric=metric,\n",
    "            update_mode=\"synchronous\",\n",
    "            transitions_order=transitions_order,\n",
    "            #### plot settings\n",
    "            ax=axes[i],\n",
    "            cbar=False if i < len(num_nodes_list) - 1 else True,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax\n",
    "        )\n",
    "\n",
    "        ### set labels\n",
    "        #### set subplot label\n",
    "        axes[i].set_title(f\"$N = {num_nodes}$\")\n",
    "        ### remove y labels on inner subplots\n",
    "        if i != 0:\n",
    "            axes[i].set_ylabel(\"\")  # usuwa \"Sampling frequency\" z pozostałych\n",
    "        #### remove singular  \n",
    "        axes[i].set_xlabel(\"\")     # usuwa \"Score function\"\n",
    "\n",
    "    # figure settings\n",
    "    ## labels\n",
    "    fig.suptitle(f'{metric} metric change \\nSynchronous update', fontsize=24)\n",
    "    fig.supxlabel(\"Score function\", fontsize=16)    \n",
    "\n",
    "    # plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wilcoxon_a = compute_wilcoxon_table(\n",
    "    dfa,\n",
    "    metrics=[\"AHD\", \"SHD\", \"SID\"],\n",
    "    transitions=[(i*20, (i+1)*20) for i in range(1, 7)],\n",
    "    group_cols=('update_mode', 'score_function', 'num_nodes'),\n",
    "    sf_col='k_value',\n",
    "    alternative='two-sided'\n",
    ")\n",
    "df_wilcoxon_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO zły podpis pltoa (lewa strona )\n",
    "# parameters\n",
    "metrics= [\"SHD\", \"AHD\", \"SID\"]\n",
    "\n",
    "# values preparation\n",
    "## Creating transition order (special sign) \n",
    "transitions_order = sorted(df_wilcoxon_a['transition'].unique(), \n",
    "       key = lambda x: int(x.split('→')[0]),\n",
    "       reverse=True)\n",
    "## get list of nodes\n",
    "num_nodes_list = df_wilcoxon_a['num_nodes'].unique()\n",
    "\n",
    "# Metric adjustment loop\n",
    "for metric in metrics:\n",
    "    ## common scale\n",
    "    df_m = df_wilcoxon_a[df_wilcoxon_a[\"metric\"] == metric]\n",
    "    abs_max = np.nanmax(np.abs(df_m[\"median_diff\"]))\n",
    "    vmin, vmax = -abs_max, abs_max\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(14, 5), sharey=True)\n",
    "    ## Single heatmap loop\n",
    "    for i, num_nodes in enumerate(num_nodes_list):\n",
    "        ### filt node value \n",
    "        filt = df_wilcoxon_a['num_nodes'] == num_nodes\n",
    "        ### add num_nodes plot \n",
    "        plot_wilcoxon_heatmap(\n",
    "            #### values\n",
    "            df_wilcoxon_a[filt],\n",
    "            metric=metric,\n",
    "            update_mode=\"asynchronous\",\n",
    "            transitions_order=transitions_order,\n",
    "            #### plot settings\n",
    "            ax=axes[i],\n",
    "            cbar=False if i < len(num_nodes_list) - 1 else True,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax\n",
    "        )\n",
    "\n",
    "        ### set labels\n",
    "        #### set subplot label\n",
    "        axes[i].set_title(f\"$N = {num_nodes}$\")\n",
    "        ### remove y labels on inner subplots\n",
    "        if i != 0:\n",
    "            axes[i].set_ylabel(\"\")  # usuwa \"Sampling frequency\" z pozostałych\n",
    "        #### remove singular  \n",
    "        axes[i].set_xlabel(\"\")     # usuwa \"Score function\"\n",
    "\n",
    "    # figure settings\n",
    "    ## labels\n",
    "    fig.suptitle(f'{metric} metric change \\nAsynchronous update', fontsize=24)\n",
    "    fig.supxlabel(\"Score function\", fontsize=16)    \n",
    "\n",
    "    # plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Experiment 4: Sensitivity to Scoring Function\n",
    "\n",
    "**Objective:**\n",
    "The aim of Experiment 4 is to evaluate how the choice of scoring function (MDL vs BDe) affects the quality of Boolean network reconstruction. We set here optimal parameters from previous experiments. That is:\n",
    "- ... te parametry wypisać \n",
    "\n",
    "**Approach:**\n",
    "Using datasets generated with fixed sampling and normalization parameters, we assess how the choice of scoring function (MDL vs BDe) influences the accuracy of Boolean network reconstruction across different network sizes and update modes. \n",
    "\n",
    "Reconstruction performance is evaluated in terms of AHD, SHD, and SID, allowing us to capture both topological accuracy and correctness of causal relationships. By systematically comparing these metrics across conditions, we aim to identify scoring function-specific tendencies and understand how the impact of the scoring criterion scales with network size and dynamical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose results from experiment 3\n",
    "## ASYNCHRONOUS: k = 100\n",
    "dfa_exp4 = dfa[dfa['k_value'] == 100]\n",
    "\n",
    "## SYNCHRONOUS:\n",
    "dfs_exp4 = dfs[\n",
    "    (\n",
    "        (dfs['num_nodes'] <= 5) & (dfs['k_value'] == 20)\n",
    "    ) |\n",
    "    (\n",
    "        (dfs['num_nodes'] > 5) & (dfs['k_value'] == 40)\n",
    "    )\n",
    "]\n",
    "\n",
    "## amount of data \n",
    "dfa_exp4.shape[0], dfs_exp4.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "metrics = [\"SHD\", \"AHD\", \"SID\"]\n",
    "\n",
    "# ---------------------\n",
    "# asynchronous case\n",
    "# ---------------------\n",
    "\n",
    "# Przygotowujemy jedną figurę z 3 kolumnami\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=False)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    df_long = dfs_exp4.melt(\n",
    "        id_vars=[\"num_nodes\", \"score_function\"], \n",
    "        value_vars=[metric], \n",
    "        var_name=\"metric\", \n",
    "        value_name=\"metric_value\"\n",
    "    )\n",
    "\n",
    "    # Rysujemy boxplot\n",
    "    plot_boxplot(\n",
    "        df=df_long,\n",
    "        x=\"num_nodes\",\n",
    "        y=\"metric_value\",\n",
    "        hue=\"score_function\",\n",
    "        title=metric,\n",
    "        ax=axes[i],\n",
    "        show_legend=(i == 2),\n",
    "        y_label=metric\n",
    "    )\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Synchronous update – Sensitivity to the scoring function (MDL vs BDe)\", \n",
    "    fontsize=18, y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------\n",
    "# asynchronous case\n",
    "# ---------------------\n",
    "\n",
    "# Przygotowujemy jedną figurę z 3 kolumnami\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=False)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    df_long = dfa_exp4.melt(\n",
    "        id_vars=[\"num_nodes\", \"score_function\"], \n",
    "        value_vars=[metric], \n",
    "        var_name=\"metric\", \n",
    "        value_name=\"metric_value\"\n",
    "    )\n",
    "\n",
    "    # Rysujemy boxplot\n",
    "    plot_boxplot(\n",
    "        df=df_long,\n",
    "        x=\"num_nodes\",\n",
    "        y=\"metric_value\",\n",
    "        hue=\"score_function\",\n",
    "        title=metric,\n",
    "        ax=axes[i],\n",
    "        show_legend=(i == 2),\n",
    "        y_label=metric\n",
    "    )\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Synchronous update – Sensitivity to the scoring function (MDL vs BDe)\", \n",
    "    fontsize=18, y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Analysis of reconstruction performance across different network sizes reveals distinct trends for the three evaluated metrics.\n",
    "\n",
    "##### Synchronous\n",
    "**Structural Hamming Distance (SHD):**\n",
    "\n",
    "The SHD median  for 5-node networks is around 5, while for 13-node networks it exceeds 10. This reflects the absolute number of edge modifications required to match the true network, which naturally grows as networks become larger and more complex.\n",
    "\n",
    "**Adjusted Hamming Distance (AHD):** \n",
    "\n",
    "A clear decreasing trend is observed as the number of nodes increases. For small networks (5 nodes), the median AHD is approximately 0.3, with values ranging from 0.2 to 0.4, whereas for larger networks (13 nodes) the median drops to around 0.1. This can indicate that, in relative terms, structural mismatches become proportionally smaller in larger networks. Important here is that model should not be overfitted as we filter out those cases.\n",
    "\n",
    "**Structural Intervention Distance (SID):** \n",
    "\n",
    "Similar to AHD, SID tends to decrease with network size. For 5-node networks, the median SID is approximately 20, whereas for 13-node networks it falls below 15. \n",
    "\n",
    "This may suggests that larger networks, despite having more nodes, show fewer errors in predicted causal effects relative to the total number of node pairs. \n",
    "\n",
    "##### Asynchronous\n",
    "* **Structural Hamming Distance (SHD):** \n",
    "\n",
    "The SHD median is increasing with number of nodes. It is caused by network size\n",
    "\n",
    "* **Adjusted Hamming Distance (AHD):**\n",
    "\n",
    "The AHD median is better with higher number of nodes. \n",
    "\n",
    "\n",
    "* **Structural Intervention Distance (SID):**\n",
    "\n",
    "\n",
    "##### Conclusion\n",
    "\n",
    "**Comparison of synchronous and asynchronous update mode**\n",
    "\n",
    "We cna\n",
    "\n",
    "Overall, these results indicate that while absolute reconstruction errors (SHD) increase with network size, relative structural and causal accuracy (AHD, SID) improves, highlighting that scoring function sensitivity remains consistent but its practical impact diminishes in larger networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "For asynchronous networks, reconstruction performance shows distinct patterns compared to synchronous dynamics:\n",
    "\n",
    "* **Adjusted Hamming Distance (AHD):** AHD decreases with network size, with a median of 0.25 for 5-node networks and a wide variability across repetitions. At 7 nodes, the median drops below 0.2, accompanied by a notably reduced spread. For larger networks, the median stabilizes slightly above 0.1, with low variability. This indicates that, under asynchronous updates, relative structural mismatches diminish as network size grows, and the reconstruction becomes more consistent across repetitions.\n",
    "\n",
    "* **Structural Hamming Distance (SHD):** SHD shows an increasing trend with network size. Starting from a median of 5 for 5-node networks, it rises to approximately 21 for the largest networks. This reflects the absolute number of edge edits required to recover the true structure, which naturally grows with network complexity, while the variability remains low.\n",
    "\n",
    "* **Structural Intervention Distance (SID):** SID remains approximately constant across network sizes, with a median around 18. This suggests that the accuracy of inferred causal relationships is relatively insensitive to network size under asynchronous dynamics.\n",
    "\n",
    "Scoring Function (MDL vs BDe): Both scoring functions for both synchronous and asynchronous updates produce virtually identical results across all metrics and network sizes, indicating that under asynchronous updates, the choice of scoring function has little effect on reconstruction outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "In this part we will compaer\n",
    "\n",
    "## Model Selection\n",
    "A validated Boolean network model was selected from the Biodivine Boolean Models repository (https://github.com/sybila/biodivine-boolean-models), with the number of nodes not exceeding 16. For this study, the following model was used:\n",
    "\n",
    "- **Model:** ERBB signaling network\n",
    "- **Source:** Biodivine repository ([model.bnet](download_models/model_id_37/model.bnet))\n",
    "- **Number of nodes:** 11\n",
    "\n",
    "## Results\n",
    "- Both MDL and BDe scoring functions were tested.\n",
    "- The inferred networks captured key regulatory relationships present in the ground truth model.\n",
    "- Precision and recall values indicate the effectiveness of the approach, with some false positives/negatives due to data limitations and model complexity.\n",
    "\n",
    "## Conclusions\n",
    "- The pipeline successfully reconstructed major features of the biological network.\n",
    "- Performance depends on data quality, trajectory diversity, and parameter settings.\n",
    "- The approach is generalizable to other biological networks of similar size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"model_id_37_merged\"\n",
    "MODELS_PATH = Path(\"download_models\")\n",
    "\n",
    "EXPERIMENT_PATH = DATA_PATH  / MODEL_NAME / 'model_id_37_merged_metrics.csv'\n",
    "model_id_37_results = pd.read_csv(EXPERIMENT_PATH)\n",
    "# bn = load_bnet_to_BN(MODELS_PATH / MODEL_NAME / \"model.bnet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sad2-final-project (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
